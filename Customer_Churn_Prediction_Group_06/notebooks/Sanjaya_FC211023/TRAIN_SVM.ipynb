{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "757536df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    ConfusionMatrixDisplay,\n",
    "    RocCurveDisplay,\n",
    "    accuracy_score,\n",
    ")\n",
    "import joblib\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d440690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_dataset_path(filename=\"churn_data.csv\", env_var=\"CHURN_DATA_PATH\", max_parent_levels=6):\n",
    "    \"\"\"Robustly find the dataset file.\n",
    "\n",
    "    Returns absolute path if found, otherwise raises FileNotFoundError.\n",
    "    \"\"\"\n",
    "    # enviorenment variable override\n",
    "    env_path = os.getenv(env_var)\n",
    "    if env_path:\n",
    "        env_path = os.path.expanduser(env_path)\n",
    "        if os.path.isfile(env_path):\n",
    "            print(f\"Using dataset from environment variable {env_var}: {env_path}\")\n",
    "            return os.path.abspath(env_path)\n",
    "        else:\n",
    "            print(f\"{env_var} is set but file not found at: {env_path}\")\n",
    "\n",
    "    # 2) try common relative paths from current working directory and parents\n",
    "    cwd = os.getcwd()\n",
    "    candidates = []\n",
    "\n",
    "    for up in range(max_parent_levels + 1):\n",
    "        base = os.path.abspath(os.path.join(cwd, *([os.pardir] * up))) if up > 0 else cwd\n",
    "        candidates.extend([\n",
    "            os.path.join(base, \"dataset\", filename),\n",
    "            os.path.join(base, \"data\", filename),\n",
    "            os.path.join(base, filename),\n",
    "            os.path.join(base, \"notebooks\", filename),\n",
    "            os.path.join(base, \"notebooks\", \"Sanjaya_FC211023\", filename),\n",
    "            os.path.join(base, \"notebooks\", \"Sanjaya_FC211023\", \"dataset\", filename),\n",
    "        ])\n",
    "\n",
    "    # also check some common mount points used in sandboxes\n",
    "    candidates.extend([\n",
    "        os.path.join(\"/workspace\", filename),\n",
    "        os.path.join(\"/workspace\", \"dataset\", filename),\n",
    "        os.path.join(\"/mnt\", \"data\", filename),\n",
    "        os.path.join(\"/mnt\", \"data\", \"dataset\", filename),\n",
    "        os.path.join(\"/home\", os.getenv(\"USER\", \"\"), filename),\n",
    "    ])\n",
    "\n",
    "    # Check candidates\n",
    "    for p in candidates:\n",
    "        if p and os.path.isfile(p):\n",
    "            print(\"Resolved dataset at:\", p)\n",
    "            return os.path.abspath(p)\n",
    "\n",
    "    # 3) shallow walk on parents (limit depth)\n",
    "    for up in range(max_parent_levels + 1):\n",
    "        base = os.path.abspath(os.path.join(cwd, *([os.pardir] * up))) if up > 0 else cwd\n",
    "        for root, dirs, files in os.walk(base):\n",
    "            if filename in files:\n",
    "                found = os.path.join(root, filename)\n",
    "                print(\"Resolved dataset via search at:\", found)\n",
    "                return os.path.abspath(found)\n",
    "\n",
    "    # Not found\n",
    "    tried = list(dict.fromkeys(candidates))  # unique preserving order\n",
    "    print(\"Tried these candidate paths:\")\n",
    "    for p in tried:\n",
    "        print(\"  -\", p)\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find '{filename}'. Set {env_var} to the absolute path or place the file in a 'dataset/' folder near your project root.\\n\"\n",
    "        f\"Current working directory: {cwd}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86617c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved dataset at: /workspaces/Customer_Churn_Prediction_Group_06/Customer_Churn_Prediction_Group_06/dataset/churn_data.csv\n",
      "\n",
      "Loading dataset from: /workspaces/Customer_Churn_Prediction_Group_06/Customer_Churn_Prediction_Group_06/dataset/churn_data.csv\n",
      "Dataset shape: (7043, 21)\n",
      "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
      "0  7590-VHVEG  Female              0     Yes         No       1           No   \n",
      "1  5575-GNVDE    Male              0      No         No      34          Yes   \n",
      "2  3668-QPYBK    Male              0      No         No       2          Yes   \n",
      "\n",
      "      MultipleLines InternetService OnlineSecurity  ... DeviceProtection  \\\n",
      "0  No phone service             DSL             No  ...               No   \n",
      "1                No             DSL            Yes  ...              Yes   \n",
      "2                No             DSL            Yes  ...               No   \n",
      "\n",
      "  TechSupport StreamingTV StreamingMovies        Contract PaperlessBilling  \\\n",
      "0          No          No              No  Month-to-month              Yes   \n",
      "1          No          No              No        One year               No   \n",
      "2          No          No              No  Month-to-month              Yes   \n",
      "\n",
      "      PaymentMethod MonthlyCharges  TotalCharges Churn  \n",
      "0  Electronic check          29.85         29.85    No  \n",
      "1      Mailed check          56.95        1889.5    No  \n",
      "2      Mailed check          53.85        108.15   Yes  \n",
      "\n",
      "[3 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "data_file = None\n",
    "try:\n",
    "    data_file = resolve_dataset_path(\"churn_data.csv\")\n",
    "except FileNotFoundError as e:\n",
    "    print(\"\\n  Dataset not found:\")\n",
    "    print(e)\n",
    "\n",
    "    if os.getenv(\"CHURN_FORCE_FAIL\") == \"1\":\n",
    "        raise\n",
    "\n",
    "    # Otherwise create a small synthetic dataset as fallback\n",
    "    print(\"\\nCreating a small synthetic dataset as a fallback so you can run the notebook end-to-end.\")\n",
    "    print(\"You should replace it with your real churn_data.csv for real results.\")\n",
    "\n",
    "    n_samples = 1000\n",
    "    X_syn, y_syn = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=8,\n",
    "        n_informative=4,\n",
    "        n_redundant=1,\n",
    "        n_clusters_per_class=1,\n",
    "        weights=[0.75, 0.25],\n",
    "        flip_y=0.01,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    df_syn = pd.DataFrame(X_syn, columns=[f\"num_feat_{i}\" for i in range(X_syn.shape[1])])\n",
    "    # Add a categorical-like feature by binning a numeric column\n",
    "    df_syn[\"contract_type\"] = pd.qcut(df_syn[\"num_feat_0\"], q=3, labels=[\"Month-to-month\", \"One year\", \"Two year\"])\n",
    "    # Add a customer id column (will be removed automatically)\n",
    "    df_syn[\"customerID\"] = [f\"C{10000 + i}\" for i in range(n_samples)]\n",
    "    df_syn[\"Churn\"] = np.where(y_syn == 1, \"Yes\", \"No\")\n",
    "\n",
    "    # Save synthetic CSV so subsequent runs pick it up too\n",
    "    fallback_path = os.path.abspath(\"_synthetic_churn.csv\")\n",
    "    df_syn.to_csv(fallback_path, index=False)\n",
    "    print(f\"Synthetic dataset written to: {fallback_path}\")\n",
    "    data_file = fallback_path\n",
    "\n",
    "# At this point data_file points to a valid CSV\n",
    "print(\"\\nLoading dataset from:\", data_file)\n",
    "df = pd.read_csv(data_file)\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(df.head(3))\n",
    "\n",
    "# Quick sanity check\n",
    "if df.shape[0] == 0 or df.shape[1] == 0:\n",
    "    raise ValueError(\"Loaded dataset appears empty. Please provide a valid churn_data.csv file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25e25fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using target column: Churn\n",
      "Dropping ID-like columns: ['customerID']\n",
      "\n",
      "Target distribution:\n",
      "Churn\n",
      "0    5174\n",
      "1    1869\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "PREFERRED_TARGET = \"Churn\"\n",
    "possible_targets = [PREFERRED_TARGET, \"churn\", \"Exited\", \"is_churn\", \"target\", \"label\"]\n",
    "\n",
    "target_col = None\n",
    "cols_lower = {c.lower(): c for c in df.columns}\n",
    "for cand in possible_targets:\n",
    "    if cand.lower() in cols_lower:\n",
    "        target_col = cols_lower[cand.lower()]\n",
    "        print(f\"Using target column: {target_col}\")\n",
    "        break\n",
    "\n",
    "if target_col is None:\n",
    "    # fall back to last column but warn\n",
    "    target_col = df.columns[-1]\n",
    "    print(f\" Could not detect 'Churn' column. Falling back to last column: {target_col}\")\n",
    "\n",
    "# Drop obvious ID columns to avoid leaking\n",
    "id_like = [c for c in df.columns if \"id\" in c.lower()]\n",
    "if id_like:\n",
    "    print(\"Dropping ID-like columns:\", id_like)\n",
    "    df = df.drop(columns=id_like)\n",
    "\n",
    "X = df.drop(columns=[target_col])\n",
    "y_raw = df[target_col]\n",
    "\n",
    "# Convert to binary 0/1\n",
    "if y_raw.dtype == object:\n",
    "    y = y_raw.astype(str).str.strip().str.lower().map(lambda v: 1 if v in {\"yes\", \"y\", \"true\", \"1\", \"churn\", \"exited\"} else 0)\n",
    "else:\n",
    "    # numeric, try to coerce\n",
    "    try:\n",
    "        y = y_raw.astype(int)\n",
    "    except Exception:\n",
    "        y = pd.Series(np.where(y_raw == y_raw.iloc[0], 0, 1), index=y_raw.index)\n",
    "\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(y.value_counts(dropna=False))\n",
    "\n",
    "# Basic validation\n",
    "unique_vals = sorted(y.unique())\n",
    "if not set(unique_vals).issubset({0, 1}):\n",
    "    raise ValueError(f\"Target values are unexpected: {unique_vals}. Please make sure target is binary (Yes/No or 0/1).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6ca7a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train shape: (5634, 19) Test shape: (1409, 19)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(\"\\nTrain shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "\n",
    "if X_train.shape[1] == 0:\n",
    "    raise ValueError(\"No feature columns found. Check your dataset columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "098a3b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features ( 3 ): ['SeniorCitizen', 'tenure', 'MonthlyCharges']\n",
      "Categorical features ( 16 ): ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'TotalCharges']\n"
     ]
    }
   ],
   "source": [
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print(\"Numeric features (\", len(numeric_features), \"):\", numeric_features)\n",
    "print(\"Categorical features (\", len(categorical_features), \"):\", categorical_features)\n",
    "\n",
    "\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", ohe),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70c80af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training SVM (this may take a bit)...\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel=\"rbf\", probability=True, class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"clf\", svm),\n",
    "])\n",
    "\n",
    "print(\"\\nTraining SVM (this may take a bit)...\")\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb6568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "try:\n",
    "    y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "except Exception:\n",
    "    # if classifier doesn't support predict_proba (shouldn't happen for SVC with probability=True)\n",
    "    y_prob = np.zeros_like(y_pred, dtype=float)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_prob) if y_prob.sum() != 0 else float(\"nan\")\n",
    "rep = classification_report(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"\\nModel performance summary:\")\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"ROC-AUC:\", auc)\n",
    "print(rep)\n",
    "\n",
    "# plots\n",
    "ConfusionMatrixDisplay(cm).plot()\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "if not np.isnan(auc):\n",
    "    RocCurveDisplay.from_predictions(y_test, y_prob)\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping ROC plot because we could not compute probabilities.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdc9fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(data_file)), os.pardir))\n",
    "outputs_dir = os.path.join(project_root, \"outputs\")\n",
    "models_dir = os.path.join(project_root, \"models\")\n",
    "os.makedirs(outputs_dir, exist_ok=True)\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "metrics = {\"accuracy\": float(acc), \"roc_auc\": (None if np.isnan(auc) else float(auc))}\n",
    "with open(os.path.join(outputs_dir, \"metrics.json\"), \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "with open(os.path.join(outputs_dir, \"classification_report.txt\"), \"w\") as f:\n",
    "    f.write(rep)\n",
    "\n",
    "model_path = os.path.join(models_dir, \"svm_churn_pipeline.joblib\")\n",
    "joblib.dump(pipeline, model_path)\n",
    "print(f\"\\nModel saved to: {model_path}\")\n",
    "print(f\"Metrics and report saved to: {outputs_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f0fe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_val = pipeline.score(X_test, y_test)\n",
    "print(\"Score() on test set:\", score_val)\n",
    "assert isinstance(score_val, float) and 0.0 <= score_val <= 1.0, \"pipeline.score must be a float between 0 and 1\"\n",
    "\n",
    "# B: predict_proba shape\n",
    "proba = pipeline.predict_proba(X_test)\n",
    "assert proba.ndim == 2 and proba.shape[1] == 2, \"predict_proba should return shape (n_samples, 2)\"\n",
    "\n",
    "# C: saved model file exists\n",
    "assert os.path.isfile(model_path), f\"Saved model not found at {model_path}\"\n",
    "\n",
    "# D: metrics file exists and contains accuracy\n",
    "with open(os.path.join(outputs_dir, \"metrics.json\"), \"r\") as f:\n",
    "    metrics_loaded = json.load(f)\n",
    "assert \"accuracy\" in metrics_loaded, \"metrics.json must contain accuracy\"\n",
    "\n",
    "# E: preview first 5 predictions with truth\n",
    "preview = X_test.copy().reset_index(drop=True).iloc[:5].copy()\n",
    "preview[\"y_true\"] = y_test.reset_index(drop=True).iloc[:5]\n",
    "preview[\"y_pred\"] = y_pred[:5]\n",
    "preview[\"p_churn\"] = y_prob[:5]\n",
    "print(\"\\nPreview (first 5):\")\n",
    "print(preview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb07fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model as Pickle File\n",
    "joblib.dump(adaboost, \"churn_svm_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
